# GPU Support Code Modifications

## Quick Reference

Once you have WSL2 + qiskit-aer-gpu set up, apply these changes to enable GPU acceleration.

---

## Changes to quantum_helpers.py

### 1. Update `simulate_split_circuits()`

Find this function and add `use_gpu` parameter:

```python
def simulate_split_circuits(sub_circuits, shots=1024, sim_method='statevector', use_gpu=False):
    """
    Simulate split sub-circuits and combine results into full bitstrings.
    
    Parameters:
    -----------
    sub_circuits : list
        List of (sub_circuit, qubit_indices) tuples from split_circuit_for_simulation
    shots : int, optional
        Number of shots for simulation
    sim_method : str, optional
        Simulation method ('statevector' or 'density_matrix')
    use_gpu : bool, optional
        If True, use GPU acceleration. Requires qiskit-aer-gpu. Default: False.
    
    Returns:
    --------
    dict: Dictionary mapping full bitstrings to counts
    """
    
    # Simulate each sub-circuit
    sub_results = []
    
    for sub_circuit, qubit_indices in sub_circuits:
        if use_gpu:
            try:
                # GPU-accelerated simulation
                simulator = AerSimulator(method=sim_method, device='GPU')
            except Exception as e:
                print(f"Warning: GPU simulation failed ({e}), falling back to CPU")
                simulator = AerSimulator(method=sim_method)
        else:
            # CPU simulation (default)
            simulator = AerSimulator(method=sim_method)
        
        transpiled_circuit = transpile(sub_circuit, simulator)
        
        job = simulator.run(transpiled_circuit, shots=shots)
        result = job.result()
        counts = result.get_counts()
        sub_results.append((counts, qubit_indices))
    
    # Combine results
    # For single-qubit gates only, each qubit's measurement is independent
    # So we randomly sample from each sub-circuit and concatenate
    combined_counts = {}
    
    for shot_idx in range(shots):
        full_bitstring = ['0'] * sum(len(indices) for _, indices in sub_results)
        
        # Sample from each sub-circuit
        for counts, qubit_indices in sub_results:
            # Sample a bitstring from this sub-circuit's distribution
            bitstrings = list(counts.keys())
            probabilities = np.array(list(counts.values())) / sum(counts.values())
            sampled_bitstring = np.random.choice(bitstrings, p=probabilities)
            
            # Place bits in correct positions
            for i, qubit_idx in enumerate(qubit_indices):
                full_bitstring[qubit_idx] = sampled_bitstring[-(i+1)]  # Qiskit uses reverse order
        
        # Convert to string and count
        full_bitstring_str = ''.join(full_bitstring)
        combined_counts[full_bitstring_str] = combined_counts.get(full_bitstring_str, 0) + 1
    
    return combined_counts
```

### 2. Update `simulate_large_circuit_in_batches()`

```python
def simulate_large_circuit_in_batches(circuit, max_qubits_per_batch=10, shots=1024, 
                                     sim_method='statevector', verbose=False, use_gpu=False):
    """
    Convenience function to split and simulate a large circuit in one call.
    
    Parameters:
    -----------
    circuit : QuantumCircuit
        The large circuit to simulate
    max_qubits_per_batch : int, optional
        Maximum qubits per batch
    shots : int, optional
        Number of shots
    sim_method : str, optional
        Simulation method
    verbose : bool, optional
        Whether to print progress messages
    use_gpu : bool, optional
        If True, use GPU acceleration. Default: False.
    
    Returns:
    --------
    dict: Combined measurement results (bitstring -> count)
    """
    
    if verbose:
        print(f"Splitting circuit with {circuit.num_qubits} qubits into batches of {max_qubits_per_batch}...")
        if use_gpu:
            print("  Using GPU acceleration")
    
    sub_circuits = split_circuit_for_simulation(circuit, max_qubits_per_batch)
    
    if verbose:
        print(f"Created {len(sub_circuits)} sub-circuits")
    
        for idx, (sub_qc, indices) in enumerate(sub_circuits):
            print(f"  Batch {idx+1}: qubits {indices[0]}-{indices[-1]} ({len(indices)} qubits)")
    
    if verbose:
        print(f"\nSimulating with {shots} shots...")
    
    results = simulate_split_circuits(sub_circuits, shots, sim_method, use_gpu=use_gpu)
    
    if verbose:
        print(f"Simulation complete! Got {len(results)} unique bitstrings")
    
    return results
```

---

## Changes to quantum_engines.py

### 1. Update `QAOA_approx()`

Add `use_gpu` parameter:

```python
def QAOA_approx(graph, graphs_dict, runtime_data, tt_data, qaoa_progress, verbose=True, layers=None, shots=None, 
                qubit_batch_size=None, inv_penalty_m=1, sim_method='statevector', label='QAOA', warm_start=None, 
                exploration_strength=0, initialization_strategy='zero', custom_initial_params=None, 
                lock_pretrained_layers=0, use_local_2q_gates=False, use_soft_validity=False, 
                soft_validity_penalty_base=10.0, use_gpu=False):
    '''
    Parameters
    ----------
    ... (all existing parameters) ...
    use_gpu : bool
        If True, use GPU acceleration for circuit simulation.
        Requires qiskit-aer-gpu package (Linux/WSL2 only). Default: False.
        Can significantly increase batch size and speed up simulations.
    
    Returns
    -------
    ... (unchanged) ...
    '''
    
    # ... (rest of function mostly unchanged) ...
    
    qaoa_result = minimize(run_QAOA,
                          x0=optimizable_init_params,
                          args=(circuit, qubit_batch_size, shots, sim_method,
                                layers, graph, qubit_to_edge_map, qaoa_results_over_time, 
                                inv_penalty, locked_params, lock_pretrained_layers,
                                use_soft_validity, soft_validity_penalty_base, use_gpu),  # ‚Üê Add use_gpu
                          method='COBYLA')
```

### 2. Update `run_QAOA()`

```python
def run_QAOA(optimizable_parameters, circuit, batch_size, shots, sim_method, layers, graph, 
             qubit_to_edge_map, results_over_time, inv_penalty=0, locked_params=None, 
             lock_pretrained_layers=0, use_soft_validity=False, soft_validity_penalty_base=10.0,
             use_gpu=False):  # ‚Üê Add use_gpu parameter
    """
    Objective function for QAOA optimization.
    
    Parameters:
    -----------
    ... (all existing parameters) ...
    use_gpu : bool
        Whether to use GPU acceleration
    """
    
    # ... (parameter reconstruction unchanged) ...
    
    bound_circuit = bind_qaoa_parameters(circuit, gamma_values, beta_values)
    counts = simulate_large_circuit_in_batches(bound_circuit, batch_size, shots, sim_method, 
                                              use_gpu=use_gpu)  # ‚Üê Pass use_gpu
    
    # ... (rest unchanged) ...
```

---

## Changes to quantum_pretraining.py

Add GPU support to pretraining:

```python
def pretrain_validity_layers(graph, qubit_to_edge_map, num_layers=1,
                            shots=1024, batch_size=8, sim_method='statevector',
                            max_iterations=50, verbose=True, use_local_2q_gates=False,
                            use_gpu=False):  # ‚Üê Add parameter
    """
    Pre-train QAOA layers together to maximize the probability of valid solutions.
    
    Parameters:
    -----------
    ... (all existing parameters) ...
    use_gpu : bool, optional
        If True, use GPU acceleration for simulations. Default: False.
    
    Returns:
    --------
    ... (unchanged) ...
    """
    
    # ... (setup unchanged) ...
    
    def validity_objective(params):
        # ... (parameter setup unchanged) ...
        
        # Bind and simulate
        bound_circuit = bind_qaoa_parameters(circuit, gamma_values, beta_values)
        counts = simulate_large_circuit_in_batches(bound_circuit, batch_size, shots, sim_method,
                                                  use_gpu=use_gpu)  # ‚Üê Pass use_gpu
        
        # ... (rest unchanged) ...
    
    # ... (rest of function unchanged) ...
```

Update `pretrain_and_create_initial_params()`:

```python
def pretrain_and_create_initial_params(graph, num_pretrain_layers=1, total_layers=3,
                                       shots=1024, batch_size=8, 
                                       max_iterations=50, verbose=True, use_local_2q_gates=False,
                                       use_gpu=False):  # ‚Üê Add parameter
    """
    ... (docstring) ...
    
    use_gpu : bool, optional
        If True, use GPU acceleration during pretraining. Default: False.
    """
    
    qubit_to_edge_map = create_qubit_to_edge_map(graph)
    
    # Pre-train specified number of layers
    pretrained_gammas, pretrained_betas, validity_rate = pretrain_validity_layers(
        graph, qubit_to_edge_map,
        num_layers=num_pretrain_layers,
        shots=shots,
        batch_size=batch_size,
        max_iterations=max_iterations,
        verbose=verbose,
        use_local_2q_gates=use_local_2q_gates,
        use_gpu=use_gpu  # ‚Üê Pass through
    )
    
    # ... (rest unchanged) ...
```

---

## Usage Examples

### Basic GPU Usage

```python
# Enable GPU for main QAOA
QAOA_approx(graph, graphs_dict, runtime_data, tt_data, qaoa_progress,
            layers=3,
            shots=10000,
            qubit_batch_size=20,  # ‚Üê Increase from 8 to 20!
            use_gpu=True,  # ‚Üê Enable GPU
            label='QAOA-GPU')
```

### GPU + All Enhancements

```python
# Pretrain with GPU
pretrained_params, validity = pretrain_and_create_initial_params(
    graph,
    num_pretrain_layers=1,
    total_layers=3,
    shots=2048,
    batch_size=20,  # ‚Üê Bigger batches
    use_local_2q_gates=True,
    use_gpu=True  # ‚Üê GPU for pretraining
)

# Main QAOA with GPU
QAOA_approx(graph, graphs_dict, runtime_data, tt_data, qaoa_progress,
            layers=3,
            shots=10000,
            qubit_batch_size=20,  # ‚Üê Bigger batches
            custom_initial_params=pretrained_params,
            lock_pretrained_layers=1,
            use_local_2q_gates=True,
            use_soft_validity=True,
            use_gpu=True,  # ‚Üê GPU for main QAOA
            label='Full-Stack-GPU')
```

### Compare CPU vs GPU

```python
qaoa_progress = {}

# CPU baseline
QAOA_approx(graph, ...,
            qubit_batch_size=8,
            use_gpu=False,
            label='CPU-8-qubits',
            qaoa_progress=qaoa_progress)

# GPU with bigger batches
QAOA_approx(graph, ...,
            qubit_batch_size=20,
            use_gpu=True,
            label='GPU-20-qubits',
            qaoa_progress=qaoa_progress)

plot_qaoa_comparison(qaoa_progress)
```

---

## Determining Optimal Batch Size

The RTX 5060 Ti has 16GB VRAM. Here's how to find your optimal batch size:

### Test Script

```python
# test_batch_sizes.py
from qiskit import QuantumCircuit
from qiskit_aer import AerSimulator
import time

def test_batch_size(num_qubits):
    """Test if a given batch size works on GPU"""
    try:
        # Create circuit
        qc = QuantumCircuit(num_qubits)
        qc.h(range(num_qubits))
        qc.measure_all()
        
        # Try GPU simulation
        simulator = AerSimulator(method='statevector', device='GPU')
        
        start = time.time()
        job = simulator.run(qc, shots=1000)
        result = job.result()
        elapsed = time.time() - start
        
        print(f"‚úì {num_qubits} qubits: {elapsed:.3f}s")
        return True
    except Exception as e:
        print(f"‚úó {num_qubits} qubits: FAILED ({str(e)[:50]})")
        return False

# Test increasing batch sizes
print("Testing batch sizes on RTX 5060 Ti (16GB VRAM)...")
for num_qubits in range(10, 30):
    if not test_batch_size(num_qubits):
        print(f"\nMax batch size: {num_qubits - 1} qubits")
        break
```

Expected result: **20-25 qubits** max batch size

### Memory Usage Estimate

| Qubits | State Vector Size | Memory | Fits in 16GB? |
|--------|------------------|--------|---------------|
| 10 | 2^10 = 1,024 | ~8 KB | ‚úì |
| 15 | 2^15 = 32,768 | ~256 KB | ‚úì |
| 20 | 2^20 = 1,048,576 | ~8 MB | ‚úì |
| 23 | 2^23 = 8,388,608 | ~67 MB | ‚úì |
| 25 | 2^25 = 33,554,432 | ~268 MB | ‚úì |
| 28 | 2^28 = 268,435,456 | ~2.1 GB | ‚úì |
| 30 | 2^30 = 1,073,741,824 | ~8.6 GB | ‚úì |

Each complex number = 8 bytes (float64), so 2^n qubits = 2^n * 8 bytes

With overhead, expect **25-28 qubit max** on 16GB VRAM.

---

## Performance Comparison

### Before (CPU, batch_size=8)
```
90 qubits total
‚Üí 12 batches of 8 qubits
‚Üí 12 √ó simulation_time
```

### After (GPU, batch_size=22)
```
90 qubits total  
‚Üí 5 batches of 18-22 qubits
‚Üí 5 √ó (simulation_time / 10)  # GPU is ~10x faster
‚Üí Overall: ~24x speedup!
```

---

## Fallback Handling

The code includes automatic fallback to CPU if GPU fails:

```python
try:
    simulator = AerSimulator(method=sim_method, device='GPU')
except Exception as e:
    print(f"Warning: GPU simulation failed ({e}), falling back to CPU")
    simulator = AerSimulator(method=sim_method)
```

This means:
- Code works on both CPU and GPU systems
- Safe to commit to version control
- Others without GPU can still run it
- You get GPU benefits when available

---

## Summary of Changes

| File | Function | Change |
|------|----------|--------|
| `quantum_helpers.py` | `simulate_split_circuits()` | Add `use_gpu` parameter |
| `quantum_helpers.py` | `simulate_large_circuit_in_batches()` | Add `use_gpu` parameter |
| `quantum_engines.py` | `QAOA_approx()` | Add `use_gpu` parameter |
| `quantum_engines.py` | `run_QAOA()` | Add `use_gpu` parameter, pass to simulator |
| `quantum_pretraining.py` | `pretrain_validity_layers()` | Add `use_gpu` parameter |
| `quantum_pretraining.py` | `pretrain_and_create_initial_params()` | Add `use_gpu` parameter |

All changes are **backward compatible** (default `use_gpu=False`).

---

## Testing Checklist

After WSL2 setup:

- [ ] `nvidia-smi` shows GPU in WSL2
- [ ] `python -c "import qiskit_aer; print(qiskit_aer.__version__)"` works
- [ ] Run `test_batch_sizes.py` to find max batch size
- [ ] Apply code modifications
- [ ] Test with `use_gpu=True, qubit_batch_size=20`
- [ ] Compare performance vs CPU
- [ ] Enjoy 10-24x speedup! üöÄ
